{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain\n",
    "LangChain is a framework that essentially makes development with large language models easier for you. It allows you to tie together various elements that you might need for an LLM-based application (vector databases, models, prompts, etc.). In a modular manner, making it much easier on you to connect everything together with very few lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (0.1.16)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (0.6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (0.0.33)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (0.1.44)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (0.1.40)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (1.25.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Community\n",
    "LangChain has split up its many components into several sub-libraries, so you may have to do a little bit of digging through the [API documentation](https://api.python.langchain.com/en/latest/langchain_api_reference.html) to find what you are looking for. A lot of what you'll be looking for can be found in [LangChain Community](https://api.python.langchain.com/en/latest/community_api_reference.html), which is the sub-library that handles things like third-party integrations. Some integrations include those of OpenAI, HuggingFace, VLLM, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import langchain_community\n",
    "\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_community.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval-Augmented Generation (RAG)\n",
    "In this demonstration, I will be introducing you to langchain by guiding you through the creation of a simplified RAG-based application. For those who aren't familiar with the topic, RAG stands for Retrieval-Augmented Generation. Like the phrase suggests, RAG is a method in which you RETRIEVE external information to AUGMENT your model to GENERATE in a certain way.\n",
    "\n",
    "Here's an example to make things a little more concrete:\n",
    "\n",
    "Imagine that you are trying to build an application with a large language model (GPT, LLaMA, Mistral, whichever you prefer) that has been pre-trained on data up to 2023. If you ask the model about the total solar eclipse that occurred on 8 Apr 2024, you couldn't possibly get an accurate answer, since the model is not psychic and cannot peer into the future. \n",
    "\n",
    "One option to fix the situation would be to train the model on current data. While this is an option, it isn't very viable, and is hardly worth the effort. The reality is that pre-training a model requires resources that most people don't have, and trying to repeatedly train a model to keep it up to date is probably not the best way to make use of your resources.\n",
    "\n",
    "Another approach you might take is connecting the model to an outside source of information, such as a website. You toss a query to the website, the website somehow finds resources that seem relevant to your query, and the LLM references those resources to create a response. This way, you don't have to worry about whether the model itself has been trained on that information; you're going to provide it with what it needs explicitly. This is an example of RAG.\n",
    "\n",
    "For now, we'll try implementing the example above using LangChain.\n",
    "\n",
    "First, we need a source of information that we can retrieve data from. While LangChain provides support for many different forms of data, we will begin with data from ArXiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  arxiv pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "attention_doc = ArxivLoader(query = '1706.03762', load_max_docs = 1).load()\n",
    "print(attention_doc[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we delve into how the data above will be used, it may be helpful to discuss the structure of a LangChain `Document` since this is the form that LangChain loaders load data with. In the above code, the ArXiv loader loads a list of LangChain documents, where each `Document` contains the contents of a query result. Because we have set the `load_max_docs` parameter to 1, `attention_doc` will only contain a single LangChain Document, corresponding to a single query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArXiv Loader return type :  <class 'list'>\n",
      "Length of return type :  1\n",
      "Type of each element :  <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(\"ArXiv Loader return type : \", type(attention_doc))\n",
    "print(\"Length of return type : \", len(attention_doc))\n",
    "print(\"Type of each element : \", type(attention_doc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LangChain `Document` is comprised of two components: `page_content` and `metadata`. Think of `page_content` as the actual text that the data originally holds. On the other hand, the `metadata` may contain all sorts of information, depending on what is available. Since the document above has been loaded from ArXiv, its `metadata` includes the paper's publication date, its title, authors, and a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Content :  Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and \n",
      "\n",
      "Metadata : \n",
      "'Published' : '2023-08-02'\n",
      "'Title' : 'Attention Is All You Need'\n",
      "'Authors' : 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin'\n",
      "'Summary' : 'The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks in an encoder-decoder configuration. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer, based\n",
      "solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to be\n",
      "superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014\n",
      "English-to-German translation task, improving over the existing best results,\n",
      "including ensembles by over 2 BLEU. On the WMT 2014 English-to-French\n",
      "translation task, our model establishes a new single-model state-of-the-art\n",
      "BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\n",
      "of the training costs of the best models from the literature. We show that the\n",
      "Transformer generalizes well to other tasks by applying it successfully to\n",
      "English constituency parsing both with large and limited training data.'\n"
     ]
    }
   ],
   "source": [
    "attention_doc = attention_doc[0]\n",
    "print(\"Page Content : \", attention_doc.page_content[:100], '\\n')\n",
    "print(\"Metadata : \")\n",
    "for key in attention_doc.metadata.keys():\n",
    "    print(f\"'{key}' : '{attention_doc.metadata[key]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a basic understanding on the type of data we will be handling, we can proceed with setting up a retrieval system for our language model. Sure, we have a source of information, but this isn't enough. Given a question from a user, we want to provide our language model with enough context to come up with an answer. Tossing the whole document in for context is hardly a solution; the length of the inputs to a language model can have drastic impacts on its generation speed and quality. \n",
    "\n",
    "In other words, we want to accurately extract the parts of the text that are relevant to a given query, and provide those to the language model instead.\n",
    "\n",
    "To do this, we first split the contents of the document into multiple sub-texts, which we will call chunks. To do this, we use the text splitters provided by LangChain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len,\n",
    "    separators = ['\\n\\n', '\\n', ' ']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following explanations (as well as many from this point on) were written to help newcomers gain an intution on how RAG works as a whole; many details, while they are important, may be omitted for sake of coherence. \n",
    "\n",
    "The `chunk_size` determines an upper bound for the length of each chunk. Again, we don't want to shove too much text into our language models for a variety of reasons.\n",
    "\n",
    "The `chunk_overlap` parameter determines how much overlapping text there should be between each chunk. Because we are splitting our original text into sub-texts based purely on length (and not, for example, by sections or chapters), there may be instances in which texts that belong together are split apart; the chunk_overlap parameter helps mitigate this by acting as a sort of buffer.\n",
    "\n",
    "An interesting thing about the `RecursiveCharacterTextSplitter` is that it takes into consideration the `chunk_size` and `separators` to look for what seem like appropriate splitting points in the original text. For those interested the details of its workings, [DEV Community](https://dev.to/eteimz/understanding-langchains-recursivecharactertextsplitter-2846) has a pretty straightforward explanation about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu' metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
      "page_content='University of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,' metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
      "page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,' metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
      "page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started' metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
      "page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and' metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n"
     ]
    }
   ],
   "source": [
    "split_docs = text_splitter.split_documents([attention_doc])\n",
    "\n",
    "# Show the first five examples\n",
    "for split_doc in split_docs[:5]:\n",
    "    print(split_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above segment of code, we can see that the original text has been successfully split into multiple chunks. One thing to note is that the `split_documents` function doesn't quite just take text and split it; we have the `split_text` function for that. Instead, the `split_documents` function takes an `Iterable` of LangChain Documents and splits those Documents into multiple new documents. So the `page_content` of each of the new Documents contains a portion of the original text, but how are the `metadata` of the new Documents created?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
      "{'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
      "{'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
      "{'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
      "{'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n"
     ]
    }
   ],
   "source": [
    "metadatas = [split_doc.metadata for split_doc in split_docs]\n",
    "\n",
    "# Show the first five examples\n",
    "for metadata in metadatas[:5]:\n",
    "    print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `metadata` of the original LangChain Document has been copied over to each of its children. So the `split_documents` function does two things\n",
    "\n",
    "1. Split the `page_content` into multiple chunks and creates a new Document for each\n",
    "2. Copy the original Document's `metadata` for all of the newly generated Documents\n",
    "\n",
    "We now have data in the form that we want (chunks), but we still don't know how to determine which chunk corresponds to a particular query. We're going to be setting up a vector database to do that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting langchain-chroma\n",
      "  Obtaining dependency information for langchain-chroma from https://files.pythonhosted.org/packages/93/a5/90225a07c5b4cf9f8390ca0ab8ce3d17c04c8135df43bf3a8675a7f0162b/langchain_chroma-0.1.0-py3-none-any.whl.metadata\n",
      "  Downloading langchain_chroma-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: chromadb<0.5.0,>=0.4.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain-chroma) (0.4.15)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain-chroma) (0.99.1)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.40 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain-chroma) (0.1.44)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain-chroma) (1.25.0)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.10.12)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.7.3)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.23.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.10.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.20.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (6.1.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.59.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.0.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (28.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (8.2.3)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.27.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.40->langchain-chroma) (6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.40->langchain-chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.40->langchain-chroma) (0.1.40)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.40->langchain-chroma) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.40->langchain-chroma) (2.1)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.23.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
      "Requirement already satisfied: urllib3<2.0,>=1.24.2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.26.16)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-chroma) (3.10.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (23.5.26)\n",
      "Requirement already satisfied: protobuf in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.20.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.11.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (6.0.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.61.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.20.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.20.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.41b0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.41b0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from requests>=2.28->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from requests>=2.28->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi<1,>=0.95.2->langchain-chroma) (3.5.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.22.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from tqdm>=4.65.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.0.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.20.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (11.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<1,>=0.95.2->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.14.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.11.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.4.8)\n",
      "Downloading langchain_chroma-0.1.0-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: langchain-chroma\n",
      "Successfully installed langchain-chroma-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-chroma\n",
    "\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Databases\n",
    "For those familiar with relational databases, think of how one queries its contents with SQL. You look for rows of data that fit the criteria detailed by your query (`age > 8`, `country == Korea`). In essence, you are looking for exact matches. With vector databases, you are looking for *similar* matches instead. For that to make more sense, we must first understand embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name = 'sentence-transformers/all-mpnet-base-v2',\n",
    "    model_kwargs = {\n",
    "        'device' : 'cpu',\n",
    "    },\n",
    "    encode_kwargs = {\n",
    "        'normalize_embeddings' : True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.030639825388789177,\n",
       " -0.00623022485524416,\n",
       " -0.0021215418819338083,\n",
       " 0.013879154808819294,\n",
       " 0.026486855000257492,\n",
       " 0.004065829794853926,\n",
       " -0.003871213411912322,\n",
       " 0.03526982665061951,\n",
       " 0.011488320305943489,\n",
       " -0.0038065307307988405,\n",
       " 0.013236396946012974,\n",
       " -0.011885696090757847,\n",
       " -0.00016037594468798488,\n",
       " 0.021570676937699318,\n",
       " 0.030149631202220917,\n",
       " -0.10559133440256119,\n",
       " 0.03541700169444084,\n",
       " 0.006596204359084368,\n",
       " -0.05530385673046112,\n",
       " -0.00022680936672259122,\n",
       " -0.007831434719264507,\n",
       " -0.004739562515169382,\n",
       " -2.1935420591034926e-05,\n",
       " 0.037050407379865646,\n",
       " 0.022426998242735863,\n",
       " 0.004295805469155312,\n",
       " -0.0010749701177701354,\n",
       " 0.007416180334985256,\n",
       " 0.011647190898656845,\n",
       " 0.01900961436331272,\n",
       " -0.01194649375975132,\n",
       " 0.0021739399526268244,\n",
       " 0.02694181725382805,\n",
       " 0.016080977395176888,\n",
       " 2.1605060283036437e-06,\n",
       " -0.008001378737390041,\n",
       " -0.013405367732048035,\n",
       " 0.016732005402445793,\n",
       " -0.009559365920722485,\n",
       " -0.022413115948438644,\n",
       " -0.0001631372288102284,\n",
       " -0.0004943871172145009,\n",
       " -0.009095557034015656,\n",
       " 0.03838304430246353,\n",
       " -0.007210048846900463,\n",
       " -0.04453025385737419,\n",
       " 0.040918730199337006,\n",
       " -0.005791069008409977,\n",
       " -0.01953091472387314,\n",
       " 0.040448158979415894,\n",
       " 0.002283487468957901,\n",
       " -0.03446432575583458,\n",
       " -0.02524823509156704,\n",
       " -0.017530670389533043,\n",
       " 0.0162602998316288,\n",
       " 0.06526771932840347,\n",
       " 0.022000562399625778,\n",
       " 0.022031625732779503,\n",
       " 0.0019334437092766166,\n",
       " -0.039544638246297836,\n",
       " 0.06365405023097992,\n",
       " -0.0023818581830710173,\n",
       " -0.018944937735795975,\n",
       " -0.029693588614463806,\n",
       " 0.052953366190195084,\n",
       " 0.04834551364183426,\n",
       " -0.017030565068125725,\n",
       " -0.0295097716152668,\n",
       " -0.005145741160959005,\n",
       " 0.04647563025355339,\n",
       " 0.003119435627013445,\n",
       " -0.007403980009257793,\n",
       " 0.02111862599849701,\n",
       " 0.0681384801864624,\n",
       " 0.0011370402062311769,\n",
       " 0.014462477527558804,\n",
       " -0.008854890242218971,\n",
       " 0.01075616292655468,\n",
       " -0.019179899245500565,\n",
       " 0.018662098795175552,\n",
       " -0.055653903633356094,\n",
       " -0.001913155778311193,\n",
       " 0.018540779128670692,\n",
       " -0.007114601321518421,\n",
       " -0.002373558236286044,\n",
       " 0.011992663145065308,\n",
       " 0.02772664465010166,\n",
       " -0.05372563377022743,\n",
       " -0.04219316318631172,\n",
       " -0.028476988896727562,\n",
       " -0.028066590428352356,\n",
       " -0.029385022819042206,\n",
       " 0.026342030614614487,\n",
       " 0.029406297951936722,\n",
       " 0.03087703138589859,\n",
       " -0.008939467370510101,\n",
       " -0.005118796601891518,\n",
       " -0.010012323036789894,\n",
       " -0.011713369749486446,\n",
       " -0.05633356794714928,\n",
       " 0.010299699380993843,\n",
       " 0.044453416019678116,\n",
       " 0.012597144581377506,\n",
       " 0.011518665589392185,\n",
       " 0.04949907585978508,\n",
       " 0.04440156742930412,\n",
       " 0.0731164962053299,\n",
       " -0.07367099076509476,\n",
       " 0.004746756050735712,\n",
       " 0.02586609125137329,\n",
       " -0.0408388152718544,\n",
       " -0.020399609580636024,\n",
       " 0.034653156995773315,\n",
       " 0.024966634809970856,\n",
       " -0.04169658198952675,\n",
       " 0.005489688832312822,\n",
       " -0.019818151369690895,\n",
       " 0.036088183522224426,\n",
       " 0.005382933653891087,\n",
       " 0.02251581661403179,\n",
       " 0.005730286240577698,\n",
       " 0.022516263648867607,\n",
       " 0.014764386229217052,\n",
       " -0.0009811314521357417,\n",
       " 0.0006267314893193543,\n",
       " -0.030046019703149796,\n",
       " -0.02463657036423683,\n",
       " 0.06529410183429718,\n",
       " -0.00664545176550746,\n",
       " -0.07179192453622818,\n",
       " 0.008428793400526047,\n",
       " 0.0012513443361967802,\n",
       " 0.005956981331110001,\n",
       " -0.01278021652251482,\n",
       " 0.07490364462137222,\n",
       " 0.05059320107102394,\n",
       " -0.03461940959095955,\n",
       " 0.019938532263040543,\n",
       " 0.061275385320186615,\n",
       " -0.0387316532433033,\n",
       " -0.030984198674559593,\n",
       " -0.0036191779654473066,\n",
       " -0.025516049936413765,\n",
       " -0.01783609762787819,\n",
       " -0.06275622546672821,\n",
       " -0.03831259906291962,\n",
       " 0.038981467485427856,\n",
       " 0.008556989021599293,\n",
       " -0.0009547668742015958,\n",
       " -0.010606867261230946,\n",
       " -0.013083265163004398,\n",
       " 0.0026401840150356293,\n",
       " -0.10052717477083206,\n",
       " 0.003781616222113371,\n",
       " 0.059111982583999634,\n",
       " 0.03312521427869797,\n",
       " 0.04542708769440651,\n",
       " -0.03813454508781433,\n",
       " -0.047896407544612885,\n",
       " -0.016921864822506905,\n",
       " -0.036989059299230576,\n",
       " 0.05555078387260437,\n",
       " 0.03160747140645981,\n",
       " -0.036022502928972244,\n",
       " 0.03169824182987213,\n",
       " -0.007501011714339256,\n",
       " 0.053730715066194534,\n",
       " -0.021916501224040985,\n",
       " -0.007846380583941936,\n",
       " -0.03851928561925888,\n",
       " -0.05391092598438263,\n",
       " 0.03783062472939491,\n",
       " -0.023017430678009987,\n",
       " -0.010004514828324318,\n",
       " 0.015116493217647076,\n",
       " 0.008228487335145473,\n",
       " -0.08045777678489685,\n",
       " 0.012932217679917812,\n",
       " 0.025701120495796204,\n",
       " -0.007122485898435116,\n",
       " -0.0081611517816782,\n",
       " -0.12638704478740692,\n",
       " 0.0036889526527374983,\n",
       " -0.028062665835022926,\n",
       " -0.026458818465471268,\n",
       " 0.0280676931142807,\n",
       " -0.022105589509010315,\n",
       " 0.05179468169808388,\n",
       " -0.036132488399744034,\n",
       " 0.022357050329446793,\n",
       " 0.027659790590405464,\n",
       " 0.008393156342208385,\n",
       " -0.06075650081038475,\n",
       " 0.0057096946984529495,\n",
       " 0.026796305552124977,\n",
       " 0.026890870183706284,\n",
       " -0.0023238372523337603,\n",
       " 0.004584436304867268,\n",
       " 0.03630392253398895,\n",
       " -0.025495825335383415,\n",
       " -0.03699652850627899,\n",
       " 0.054565977305173874,\n",
       " 0.03090897761285305,\n",
       " -0.04290398210287094,\n",
       " -0.016337675973773003,\n",
       " 0.016842445358633995,\n",
       " 0.07048442214727402,\n",
       " 0.001217615557834506,\n",
       " -0.014694486744701862,\n",
       " 0.033129457384347916,\n",
       " -0.005192540120333433,\n",
       " -0.01735677570104599,\n",
       " 0.061018720269203186,\n",
       " 0.024660831317305565,\n",
       " 0.004231706727296114,\n",
       " 0.044130824506282806,\n",
       " 0.014588668942451477,\n",
       " 0.05558547005057335,\n",
       " 0.016129344701766968,\n",
       " 0.05278884992003441,\n",
       " 0.015354015864431858,\n",
       " 0.03420222923159599,\n",
       " 0.09391707926988602,\n",
       " -0.0019694853108376265,\n",
       " 0.09109330177307129,\n",
       " -0.06517070531845093,\n",
       " 0.005039221607148647,\n",
       " 0.0017743349308148026,\n",
       " 1.1020888450730126e-05,\n",
       " 0.037222348153591156,\n",
       " -0.005997039843350649,\n",
       " 0.000472125131636858,\n",
       " -0.01059128250926733,\n",
       " 0.018768414855003357,\n",
       " 0.006095064803957939,\n",
       " 0.027544425800442696,\n",
       " 0.1033165454864502,\n",
       " -0.05722901597619057,\n",
       " 0.012514299713075161,\n",
       " -0.06303069740533829,\n",
       " 0.03612462803721428,\n",
       " 0.007395182736217976,\n",
       " -0.0017696942668408155,\n",
       " -0.07381031662225723,\n",
       " 0.08106907457113266,\n",
       " 0.007211185060441494,\n",
       " -0.0007785021443851292,\n",
       " 0.007916701957583427,\n",
       " 0.007236341014504433,\n",
       " -0.02021566964685917,\n",
       " -0.036108676344156265,\n",
       " -0.008541138842701912,\n",
       " -0.035862285643815994,\n",
       " 0.008248456753790379,\n",
       " 0.012955404818058014,\n",
       " -0.027108509093523026,\n",
       " -0.03072391264140606,\n",
       " -0.02438952401280403,\n",
       " -0.06183725595474243,\n",
       " -0.06575818359851837,\n",
       " 0.013959119096398354,\n",
       " -0.0348648838698864,\n",
       " 0.0023419205099344254,\n",
       " 0.0061063324101269245,\n",
       " 0.016339126974344254,\n",
       " 0.025853199884295464,\n",
       " 0.0034618901554495096,\n",
       " -0.0792006254196167,\n",
       " -0.012546001933515072,\n",
       " -0.02183409407734871,\n",
       " 0.04294608160853386,\n",
       " -0.0016075322637334466,\n",
       " 0.01813068427145481,\n",
       " 0.000740352610591799,\n",
       " -0.024322116747498512,\n",
       " -0.020828476175665855,\n",
       " 0.055237576365470886,\n",
       " 0.004020963795483112,\n",
       " -0.028246864676475525,\n",
       " 0.05310354754328728,\n",
       " 0.015112404711544514,\n",
       " -0.002685349900275469,\n",
       " -0.06379347294569016,\n",
       " 0.0003912733809556812,\n",
       " 0.019577832892537117,\n",
       " 0.03228354454040527,\n",
       " 0.04972062632441521,\n",
       " -0.06776180118322372,\n",
       " 0.03693244606256485,\n",
       " -0.014265898615121841,\n",
       " -0.055941350758075714,\n",
       " 0.01670999825000763,\n",
       " -0.0034831748344004154,\n",
       " -0.04910140112042427,\n",
       " 0.052292484790086746,\n",
       " -0.004457525908946991,\n",
       " 0.006998548284173012,\n",
       " -0.0425676591694355,\n",
       " 0.019462866708636284,\n",
       " 0.042803287506103516,\n",
       " -0.06234077736735344,\n",
       " 0.061667557805776596,\n",
       " 0.005652435589581728,\n",
       " 0.0030656831804662943,\n",
       " 0.006493190303444862,\n",
       " -0.01802985370159149,\n",
       " 0.01704832911491394,\n",
       " -0.07834608852863312,\n",
       " -0.005906788632273674,\n",
       " 0.030991848558187485,\n",
       " 0.03886578604578972,\n",
       " 0.02607213892042637,\n",
       " 0.020868821069598198,\n",
       " 0.02753918059170246,\n",
       " 0.03540617972612381,\n",
       " -0.03304383531212807,\n",
       " -0.027661127969622612,\n",
       " -0.017667219042778015,\n",
       " 0.06269259750843048,\n",
       " -0.02308349497616291,\n",
       " 0.003992971498519182,\n",
       " 0.009904647246003151,\n",
       " 0.038875747472047806,\n",
       " 0.01563560590147972,\n",
       " -0.049270328134298325,\n",
       " 0.08511310815811157,\n",
       " -0.03880713880062103,\n",
       " 0.024810345843434334,\n",
       " 0.019371677190065384,\n",
       " 0.031206417828798294,\n",
       " 0.02229176089167595,\n",
       " 0.06424488872289658,\n",
       " 0.007690731436014175,\n",
       " -0.021345417946577072,\n",
       " -0.05383220687508583,\n",
       " 0.004644749686121941,\n",
       " 0.02366814576089382,\n",
       " -0.05730965733528137,\n",
       " 0.0038594775833189487,\n",
       " -0.020337533205747604,\n",
       " -0.0324598103761673,\n",
       " -0.049819834530353546,\n",
       " -0.019346771761775017,\n",
       " -0.021453889086842537,\n",
       " -0.05271685868501663,\n",
       " -0.03349679708480835,\n",
       " -0.006598688196390867,\n",
       " -0.026178326457738876,\n",
       " -0.08520960807800293,\n",
       " -0.0015887776389718056,\n",
       " -0.08294134587049484,\n",
       " -0.044514477252960205,\n",
       " 0.022526592016220093,\n",
       " -0.029956737533211708,\n",
       " 0.0033579515293240547,\n",
       " -0.0013394539710134268,\n",
       " -0.021021611988544464,\n",
       " 0.010683215223252773,\n",
       " -0.010258650407195091,\n",
       " 4.600915053742938e-05,\n",
       " -0.009262610226869583,\n",
       " -0.022609934210777283,\n",
       " 0.022403210401535034,\n",
       " 0.05562214553356171,\n",
       " -0.006611919030547142,\n",
       " -0.000935878255404532,\n",
       " 0.017431747168302536,\n",
       " 0.03107316792011261,\n",
       " -0.0325845442712307,\n",
       " -0.008992540650069714,\n",
       " -0.011195432394742966,\n",
       " 0.04186954349279404,\n",
       " -0.03934190422296524,\n",
       " 0.07081752270460129,\n",
       " 0.008358811028301716,\n",
       " -0.009886539541184902,\n",
       " -0.0442449226975441,\n",
       " -0.04294427111744881,\n",
       " 0.050863832235336304,\n",
       " -0.04244345799088478,\n",
       " 0.036188650876283646,\n",
       " -0.04363774135708809,\n",
       " 0.008005146868526936,\n",
       " -0.022846141830086708,\n",
       " -0.001095707411877811,\n",
       " -0.028862906619906425,\n",
       " 0.017813561484217644,\n",
       " -0.004539033863693476,\n",
       " -0.020015573129057884,\n",
       " 0.0034995037131011486,\n",
       " 0.029294302687048912,\n",
       " -0.05230259150266647,\n",
       " -0.025006409734487534,\n",
       " 0.007852940820157528,\n",
       " 0.01609712839126587,\n",
       " 0.05712172016501427,\n",
       " 0.03930649906396866,\n",
       " 0.039293672889471054,\n",
       " -0.04498358443379402,\n",
       " 0.05681793391704559,\n",
       " -0.025631336495280266,\n",
       " 0.05762653052806854,\n",
       " 0.041467685252428055,\n",
       " 0.0007624694844707847,\n",
       " 0.04789058491587639,\n",
       " 0.025518331676721573,\n",
       " 0.0665455162525177,\n",
       " 0.02722831629216671,\n",
       " -0.010134860873222351,\n",
       " 0.03422801196575165,\n",
       " -0.06002845615148544,\n",
       " -0.019918905571103096,\n",
       " -0.019405093044042587,\n",
       " -0.031721752136945724,\n",
       " -0.025831298902630806,\n",
       " -0.03604121878743172,\n",
       " -0.020421508699655533,\n",
       " -0.06601198762655258,\n",
       " -0.0015701933298259974,\n",
       " 0.010516501031816006,\n",
       " 0.01943216286599636,\n",
       " -0.020242907106876373,\n",
       " -0.02712646685540676,\n",
       " 0.023355118930339813,\n",
       " -0.052650220692157745,\n",
       " 0.01886596716940403,\n",
       " 0.06436800956726074,\n",
       " 0.0884583443403244,\n",
       " 0.018637627363204956,\n",
       " -0.016322864219546318,\n",
       " 0.028206760063767433,\n",
       " -0.04460794851183891,\n",
       " 0.0381879024207592,\n",
       " 0.06300438940525055,\n",
       " -0.006718631833791733,\n",
       " 0.042649272829294205,\n",
       " -0.021121066063642502,\n",
       " 0.02563788928091526,\n",
       " -0.10721509903669357,\n",
       " -0.048154983669519424,\n",
       " -0.009137541055679321,\n",
       " -0.06949079036712646,\n",
       " 0.0275157131254673,\n",
       " -0.009887639433145523,\n",
       " 0.014211799949407578,\n",
       " 0.027101576328277588,\n",
       " 0.041130758821964264,\n",
       " 0.010981235653162003,\n",
       " 0.017674868926405907,\n",
       " -0.009279947727918625,\n",
       " 0.011950002983212471,\n",
       " -0.04461251571774483,\n",
       " 0.015854088589549065,\n",
       " -0.030766919255256653,\n",
       " 0.016769587993621826,\n",
       " -0.05685129761695862,\n",
       " -0.06606966257095337,\n",
       " 0.014340586960315704,\n",
       " 0.029277494177222252,\n",
       " -0.010260237380862236,\n",
       " 0.09664896130561829,\n",
       " 0.038746654987335205,\n",
       " -0.028536096215248108,\n",
       " -0.003963689785450697,\n",
       " 0.007153284270316362,\n",
       " -0.05481778830289841,\n",
       " -0.06089898571372032,\n",
       " 0.05530673637986183,\n",
       " 0.005109685007482767,\n",
       " -0.008417836390435696,\n",
       " -0.11410747468471527,\n",
       " -0.005880679469555616,\n",
       " -0.019880471751093864,\n",
       " -0.029771417379379272,\n",
       " 4.763900869875215e-05,\n",
       " 0.05950307101011276,\n",
       " -0.06165071949362755,\n",
       " -0.07923293858766556,\n",
       " -0.05610688775777817,\n",
       " -0.06013166531920433,\n",
       " -0.022108128294348717,\n",
       " -0.061434000730514526,\n",
       " 0.007680386770516634,\n",
       " 0.06323789060115814,\n",
       " 0.024356653913855553,\n",
       " -0.04770383983850479,\n",
       " 0.03481912612915039,\n",
       " -0.028963807970285416,\n",
       " 0.0002774869790300727,\n",
       " -0.033022698014974594,\n",
       " -0.008429668843746185,\n",
       " 0.0303533673286438,\n",
       " 0.05723147839307785,\n",
       " -0.00042322787339799106,\n",
       " -0.039693683385849,\n",
       " 0.08210484683513641,\n",
       " 0.047146495431661606,\n",
       " -0.02956032194197178,\n",
       " -0.06181538105010986,\n",
       " 0.042275458574295044,\n",
       " -0.03773244470357895,\n",
       " 0.03479821979999542,\n",
       " 0.0034918582532554865,\n",
       " -0.014311891980469227,\n",
       " -0.0010727877961471677,\n",
       " -0.02509807050228119,\n",
       " -0.021008620038628578,\n",
       " 0.003972402773797512,\n",
       " 0.016544004902243614,\n",
       " -0.029297731816768646,\n",
       " 0.023720255121588707,\n",
       " 0.02423597127199173,\n",
       " -0.006201918702572584,\n",
       " -0.08808878809213638,\n",
       " 0.030963703989982605,\n",
       " 0.06882385164499283,\n",
       " -0.02659805491566658,\n",
       " -0.010301215574145317,\n",
       " -0.02233533188700676,\n",
       " -0.052441492676734924,\n",
       " 0.022985242307186127,\n",
       " 0.0002516297099646181,\n",
       " 0.010531789623200893,\n",
       " -0.016295744106173515,\n",
       " 0.03977831453084946,\n",
       " 0.03831765800714493,\n",
       " 0.01038782112300396,\n",
       " -0.03611716628074646,\n",
       " 0.01852516457438469,\n",
       " 0.018565436825156212,\n",
       " -0.0622139573097229,\n",
       " -0.020730359479784966,\n",
       " 0.049839939922094345,\n",
       " 0.003855828894302249,\n",
       " -0.041873760521411896,\n",
       " 0.06582378596067429,\n",
       " 0.024094119668006897,\n",
       " 0.06296300888061523,\n",
       " -0.03551190346479416,\n",
       " 0.0415823794901371,\n",
       " 0.003934770356863737,\n",
       " 0.03850001469254494,\n",
       " -0.005495078861713409,\n",
       " -0.018632609397172928,\n",
       " 0.024550795555114746,\n",
       " 0.08363858610391617,\n",
       " 0.01689644157886505,\n",
       " 0.002815453102812171,\n",
       " -0.01615394465625286,\n",
       " -0.014512717723846436,\n",
       " -0.007635190617293119,\n",
       " 0.042408186942338943,\n",
       " -0.023359615355730057,\n",
       " 0.013670124113559723,\n",
       " -0.017844511196017265,\n",
       " -8.499708715944132e-33,\n",
       " -0.021985486149787903,\n",
       " 0.021068403497338295,\n",
       " -0.06685961782932281,\n",
       " -0.02435939759016037,\n",
       " -0.017444660887122154,\n",
       " -0.06685099750757217,\n",
       " 0.02519608661532402,\n",
       " -0.016856415197253227,\n",
       " -0.015659725293517113,\n",
       " 0.019933387637138367,\n",
       " -0.025513824075460434,\n",
       " -0.001185451284982264,\n",
       " 0.03305712342262268,\n",
       " 0.009693673811852932,\n",
       " -0.0033242981880903244,\n",
       " -0.010492048226296902,\n",
       " 0.07247306406497955,\n",
       " 0.03223366290330887,\n",
       " -0.02368069626390934,\n",
       " -8.159426943166181e-05,\n",
       " 0.014859518967568874,\n",
       " 0.013497856445610523,\n",
       " 0.04198714718222618,\n",
       " 0.03127151355147362,\n",
       " 0.08609764277935028,\n",
       " 0.0018721832893788815,\n",
       " -0.029854007065296173,\n",
       " 0.002177272457629442,\n",
       " -0.006728203035891056,\n",
       " 0.003352342639118433,\n",
       " -0.056060902774333954,\n",
       " 0.006746857427060604,\n",
       " -0.005793447606265545,\n",
       " 0.04290587455034256,\n",
       " 0.011770274490118027,\n",
       " 0.055365487933158875,\n",
       " -0.03638426959514618,\n",
       " -0.024264710023999214,\n",
       " 0.032628949731588364,\n",
       " 0.01567288488149643,\n",
       " 0.022836288437247276,\n",
       " 0.005584592465311289,\n",
       " -0.008583717048168182,\n",
       " -0.04995222017168999,\n",
       " -0.05311371758580208,\n",
       " 0.033992521464824677,\n",
       " -0.0026507284492254257,\n",
       " 0.015019966289401054,\n",
       " 0.04506692290306091,\n",
       " -0.024675769731402397,\n",
       " -0.07944446057081223,\n",
       " -0.02399529330432415,\n",
       " -0.010769639164209366,\n",
       " -0.03450673818588257,\n",
       " -0.04937329515814781,\n",
       " -0.027021009474992752,\n",
       " 0.0050038620829582214,\n",
       " -0.007323032710701227,\n",
       " 0.01968623697757721,\n",
       " -0.014944625087082386,\n",
       " 0.032781582325696945,\n",
       " -0.0314357727766037,\n",
       " -0.0598490834236145,\n",
       " 0.04352223873138428,\n",
       " 0.03758998215198517,\n",
       " 0.020767904818058014,\n",
       " 0.14464671909809113,\n",
       " 0.0008991016657091677,\n",
       " -0.02504447102546692,\n",
       " -0.015120538882911205,\n",
       " -0.023547468706965446,\n",
       " -0.009156213141977787,\n",
       " 0.009094043634831905,\n",
       " -0.009519385173916817,\n",
       " -0.034162022173404694,\n",
       " -0.03853987529873848,\n",
       " -0.07852219045162201,\n",
       " -0.026674550026655197,\n",
       " 0.012151487171649933,\n",
       " -0.003438404994085431,\n",
       " -0.0324154756963253,\n",
       " 0.007012520916759968,\n",
       " 0.0024134127888828516,\n",
       " 0.02128170244395733,\n",
       " 0.016268901526927948,\n",
       " 0.01879732683300972,\n",
       " -0.011541878804564476,\n",
       " -0.01415177807211876,\n",
       " -0.07110940665006638,\n",
       " -0.023764943704009056,\n",
       " -0.02890801429748535,\n",
       " -0.038899682462215424,\n",
       " -0.007442557718604803,\n",
       " 0.029943563044071198,\n",
       " -0.016707979142665863,\n",
       " -0.0513879731297493,\n",
       " -0.006651288829743862,\n",
       " -0.01970417983829975,\n",
       " 0.012060170993208885,\n",
       " -0.01566777192056179,\n",
       " -0.03231058642268181,\n",
       " 0.04340871796011925,\n",
       " -0.04691489040851593,\n",
       " 0.0078068869188427925,\n",
       " 0.017847906798124313,\n",
       " 0.05960361659526825,\n",
       " -0.03275097534060478,\n",
       " 0.009308000095188618,\n",
       " -0.05042341351509094,\n",
       " -0.02107705920934677,\n",
       " 0.028273407369852066,\n",
       " 0.04854509234428406,\n",
       " 0.05735188350081444,\n",
       " 0.04344511404633522,\n",
       " -0.02769053913652897,\n",
       " 0.04622684046626091,\n",
       " 0.018540270626544952,\n",
       " -0.03519546985626221,\n",
       " -0.043240129947662354,\n",
       " -0.00039436828228645027,\n",
       " -0.029420239850878716,\n",
       " 0.024749794974923134,\n",
       " -0.025479381904006004,\n",
       " -0.03791432827711105,\n",
       " -0.02942250669002533,\n",
       " -0.03681088238954544,\n",
       " -0.0028028800152242184,\n",
       " 0.05293608084321022,\n",
       " -0.0009389136685058475,\n",
       " -0.017110563814640045,\n",
       " 0.03922624513506889,\n",
       " -0.011791978031396866,\n",
       " 2.600843629352312e-07,\n",
       " 0.032921772450208664,\n",
       " -0.02308637462556362,\n",
       " -0.023671003058552742,\n",
       " 0.08377307653427124,\n",
       " -0.01405001524835825,\n",
       " 0.026652153581380844,\n",
       " 0.02284187078475952,\n",
       " 0.0539705865085125,\n",
       " 0.010514998808503151,\n",
       " 0.030627630650997162,\n",
       " -0.018788525834679604,\n",
       " -0.03036382794380188,\n",
       " 0.01585688441991806,\n",
       " -0.04756969213485718,\n",
       " -0.02773524448275566,\n",
       " -0.10218365490436554,\n",
       " -0.036413490772247314,\n",
       " -0.10162025690078735,\n",
       " -0.02361377514898777,\n",
       " -0.03513319045305252,\n",
       " -0.002442971570417285,\n",
       " 0.016642920672893524,\n",
       " 0.0014042293187230825,\n",
       " -0.030491475015878677,\n",
       " -0.009621471166610718,\n",
       " 0.0005119303241372108,\n",
       " 0.023772183805704117,\n",
       " -0.04487713426351547,\n",
       " 0.03109705075621605,\n",
       " 0.005117211025208235,\n",
       " 0.03554345667362213,\n",
       " -0.010703794658184052,\n",
       " 0.011081253178417683,\n",
       " -0.012116939760744572,\n",
       " 0.024507667869329453,\n",
       " -0.023132199421525,\n",
       " 0.01394661795347929,\n",
       " 0.018509451299905777,\n",
       " -0.019457846879959106,\n",
       " 0.06511240452528,\n",
       " -0.04875820502638817,\n",
       " 0.005255820229649544,\n",
       " -0.020825745537877083,\n",
       " -0.013219957239925861,\n",
       " 0.04433064162731171,\n",
       " -0.015206377021968365,\n",
       " 0.0005758269689977169,\n",
       " 0.0029455693438649178,\n",
       " -0.06738471984863281,\n",
       " -0.030147965997457504,\n",
       " -0.01855357550084591,\n",
       " -0.020780257880687714,\n",
       " -0.004518046975135803,\n",
       " 0.01933162286877632,\n",
       " -0.023686232045292854,\n",
       " 0.05517202615737915,\n",
       " 0.05380627140402794,\n",
       " 0.011861028149724007,\n",
       " 0.029966074973344803,\n",
       " 0.035616274923086166,\n",
       " -0.03322616592049599,\n",
       " -0.05000747740268707,\n",
       " -0.0018625372322276235,\n",
       " 0.015244371257722378,\n",
       " -0.04828120023012161,\n",
       " 0.015600922517478466,\n",
       " -0.015480775386095047,\n",
       " 7.099239141588482e-35,\n",
       " -0.009518262930214405,\n",
       " 0.04195930436253548,\n",
       " 0.03483816981315613,\n",
       " 0.08632058650255203,\n",
       " 0.022271176800131798,\n",
       " -0.005905501078814268,\n",
       " -0.08025659620761871,\n",
       " -0.009358665905892849,\n",
       " 0.03398369252681732,\n",
       " -0.01675461232662201,\n",
       " 0.005198839120566845]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.embed_query('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As AWS puts it, [\"Embedding models are algorithms trained to encapsulate information into dense representations in multi-dimensional space.\"](https://aws.amazon.com/what-is/embeddings-in-machine-learning/?nc1=h_ls) If you aren't familiar with vectors and higher-dimensional spaces, think of embedding models as algorithms that convert text into coordinates in the x-y plane. Somehow, the embedding model knows that points representing words or phrases similar or relevant to each other ('dog' and 'cat') should be placed nearby and that those that aren't should be spaced further apart ('giraffe' and 'tank'). [This image](https://miro.medium.com/v2/resize:fit:850/1*jptD3Rur8gUOftw-XHrezQ.png) from [this article](https://medium.com/@eugene-s/the-rise-of-embedding-technology-and-vector-databases-in-ai-4a8db58eb332) about embeddings might help if you're struggling to make sense of what that would look like.\n",
    "\n",
    "So those decimals in that cell above are actually the coordinates for a vector (or point if you would prefer) in an incredibly high-dimensional space. Keep that in mind, but don't bother trying to picture it; thinking of the 3D or 2D alternative works fine for our purposes.\n",
    "\n",
    "What's nice about embedding models (and the vector embeddings that they create) is that they provide machines with a meaningful way to make comparisons. More specifically, it's obvious to us that the words 'child' and 'parent' have some sort of relationship, but a machine doesn't know that. If you give it two vector embeddings, however, a machine can tell you things like how far apart the two are and THAT can be used as concrete indicator of things like 'relevance' or 'similarity' - the smaller the distance between them, the closer the two vectors are, and the more likely they are to be relevant with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector database (Chroma) using the ArXiv documents and embedding model from above\n",
    "chromadb = Chroma.from_documents(\n",
    "    documents = split_docs,\n",
    "    embedding = embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector databases work by representing the data contained within with vector embeddings. Think of them as a huge collection of `keys` and `values`. When data is added to a vector database, the data (corresponding to the `values`) is first converted into a vector embedding (`keys`) as you've seen above. These vector embeddings act as the 'key' that represent the data that has been added. So instead of looking for exact matches like a relational database, vector databases work by figuring out which entries seem most similar or relevant to the query (their vector embeddings are closest to the query).\n",
    "\n",
    "The search process, summarized, can be thought of as follows:\n",
    "1. Query is provided by user\n",
    "2. Query is converted into a vector embedding\n",
    "3. The vector database somehow identifies the `keys` (vectors) that are closest to the query vector embedding\n",
    "4. The vector database returns the `values` associated with those `keys`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.3\n",
      "Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "{'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Published': '2023-08-02', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.', 'Title': 'Attention Is All You Need'}\n"
     ]
    }
   ],
   "source": [
    "retrieved_document = chromadb.similarity_search('how is attention implemented in a transformers model?', k = 1)\n",
    "print(retrieved_document[0].page_content)\n",
    "print(retrieved_document[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a simple search seems to yield decent results; at the very least, the document returned by the vector database doesn't seem completely unrelated to the query we've provided.\n",
    "\n",
    "So far we've done the following:\n",
    "1. Select the data to use for querying\n",
    "    - Attention is All You Need paper\n",
    "2. Prepared the data in a way that our language model can use\n",
    "    - Split text into chunks\n",
    "3. Prepared a way to select which of the data to hand over to the language model\n",
    "    - The vector database does this for us using an embedding model\n",
    "\n",
    "So now all that's left is to hand this information over to a language model that can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cwpar\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n\u001b[0;32m      4\u001b[0m pretrained_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "pretrained_model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(\n",
    "    pipeline = pipeline(\n",
    "        'text-generation',\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name\n",
    "        ),\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell above may not make much sense to those unfamilar with the HuggingFace libraries, but the details for that are well beyond the scope of this introductory tutorial. For now, all that's important is that the `pipeline` that has been constructed above allows you to toss text into a language model and receive text back. Things may have been easier with an OpenAI model (since LangChain provides integrations for that), but I figured that for this particular tutorial, I should probably focus on models that are free to. \n",
    "\n",
    "The language model that does our generations is [neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3), a model that has been fine-tuned by Intel. It will take a while to load, because large language models are quite literally massive.\n",
    "\n",
    "Once the model has finished loading, we can test how it performs with normal, everyday chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "c:\\Users\\cwpar\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Human: What are some members of Sesame Street?\\n\\nAI: Some members of S'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_pipeline.invoke('what is the ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
