{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain\n",
    "LangChain is a framework that essentially makes development with large language models easier for you. It allows you to tie together various elements that you might need for an LLM-based application (vector databases, models, prompts, etc.). In a modular manner, making it much easier on you to connect everything together with very few lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Community\n",
    "LangChain has split up its many components into several sub-libraries, so you may have to do a little bit of digging through the [API documentation](https://api.python.langchain.com/en/latest/langchain_api_reference.html) to find what you are looking for. A lot of what you'll be looking for can be found in [LangChain Community](https://api.python.langchain.com/en/latest/community_api_reference.html), which is the sub-library that handles things like third-party integrations. Some integrations include those of OpenAI, HuggingFace, VLLM, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import langchain_community\n",
    "\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_community.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval-Augmented Generation (RAG)\n",
    "In this demonstration, I will be introducing you to langchain by guiding you through the creation of a simplified RAG-based application. For those who aren't familiar with the topic, RAG stands for Retrieval-Augmented Generation. Like the phrase suggests, RAG is a method in which you RETRIEVE external information to AUGMENT your model to GENERATE in a certain way.\n",
    "\n",
    "Here's an example to make things a little more concrete:\n",
    "\n",
    "Imagine that you are trying to build an application with a large language model (GPT, LLaMA, Mistral, whichever you prefer) that has been pre-trained on data up to 2023. If you ask the model about the total solar eclipse that occurred on 8 Apr 2024, you couldn't possibly get an accurate answer, since the model is not psychic and cannot peer into the future. \n",
    "\n",
    "One option to fix the situation would be to train the model on current data. While this is an option, it isn't very viable, and is hardly worth the effort. The reality is that pre-training a model requires resources that most people don't have, and trying to repeatedly train a model to keep it up to date is probably not the best way to make use of your resources.\n",
    "\n",
    "Another approach you might take is connecting the model to an outside source of information, such as a website. You toss a query to the website, the website somehow finds resources that seem relevant to your query, and the LLM references those resources to create a response. This way, you don't have to worry about whether the model itself has been trained on that information; you're going to provide it with what it needs explicitly. This is an example of RAG.\n",
    "\n",
    "For now, we'll try implementing the example above using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
